{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwritten Digit Recognition Project 🚀\n",
        "\n",
        "Welcome to our exciting journey into the world of Deep Learning! In this project, you'll dive into the fascinating realm of handwritten digit recognition using PyTorch, one of the most popular machine learning libraries. 🧠💻\n",
        "\n",
        "## Project Overview 📝\n",
        "\n",
        "Your mission, should you choose to accept it, involves building and optimizing a PyTorch model to recognize handwritten digits from the MNIST dataset. This dataset is like the 'Hello World' of machine learning, perfect for beginners and yet intriguing for experienced coders. 🌟\n",
        "\n",
        "## Learning Objectives 🎯\n",
        "\n",
        "- **Understanding PyTorch**: Get hands-on experience with PyTorch, understanding its basic operations and how to build models with it.\n",
        "- **Model Optimization**: Explore various training optimization techniques such as adding dropout layers, implementing regularizers, and utilizing early stopping to enhance model performance.\n",
        "- **Experimentation**: Test different hyperparameters and observe how they impact your model's learning process and accuracy.\n",
        "\n",
        "## Project Structure 🗂️\n",
        "\n",
        "- **Data Preprocessing**: Learn how to prepare your data for optimal model training.\n",
        "- **Model Building**: Design a neural network architecture suitable for digit recognition.\n",
        "- **Training and Testing**: Implement the training loop, and test your model's performance.\n",
        "- **Optimization Techniques**: Apply different optimization strategies to improve your model.\n",
        "\n",
        "## TODOs 📌\n",
        "\n",
        "Throughout this notebook, you'll find `TODO` sections. These are areas where you'll need to apply what you've learned and write your own code. Don't worry, though; guidance and hints are provided to help you on your journey!\n",
        "\n",
        "So, are you ready to embark on this adventure in machine learning? Let's get started! 🚀👩‍💻👨‍💻\n",
        "\n",
        "---\n",
        "\n",
        "Remember, the goal of this project is not just to build a model but to experiment and learn. Every challenge you encounter is an opportunity to grow. Let's do this! 💪\n"
      ],
      "metadata": {
        "id": "NAsgDbOhyWgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the MNIST Dataset 📚\n",
        "\n",
        "Before diving into the model building, the first crucial step is to load our dataset. In this section, you'll learn how to load and visualize the MNIST dataset, which is a collection of 70,000 grayscale images of handwritten digits (0 through 9). This dataset is widely used for training and testing in the field of machine learning. 🤖📈\n",
        "\n"
      ],
      "metadata": {
        "id": "ivgWP56HytSS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r2xTBOMDx53Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad829cf-250f-47d1-8fb9-fbbc51e966fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 14772673.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 443235.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3931268.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2414821.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TODO: Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    # TODO: Add necessary transformations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# TODO: Load the MNIST dataset\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# TODO: Create data loaders\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Figure out how many images are in the train_set and test_set.\n"
      ],
      "metadata": {
        "id": "w3OXyKQjy_zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VgCpxjs460F",
        "outputId": "01a9cc48-3f0c-4a7e-eaaa-e732958b30b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEsZjbAG5DhX",
        "outputId": "e3a1ddd3-cb52-4feb-dcd0-f9082eae057c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Neural Network Model 🛠️\n",
        "\n",
        "Now that our dataset is ready, it's time to build the neural network model that will learn to recognize handwritten digits. In this section, you will define the architecture of your neural network.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Layers**: Neural networks are composed of layers. Each layer has a specific role, like convolutional layers for feature extraction or fully connected (dense) layers for decision making.\n",
        "- **Activation Functions**: These functions introduce non-linear properties to the network, allowing it to learn more complex patterns."
      ],
      "metadata": {
        "id": "LKFfUT-ozFA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_set[0]\n",
        "print(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BZU19S65Wbw",
        "outputId": "7bb862c1-36a1-4c38-d1a6-fe3f02eeaa76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PyTorch libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Define the neural network class\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # TODO: Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(784,10 ) # First fully connected layer\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)  # TODO: add an activation function\n",
        "        x =F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "SDoUc4dzzZMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443690ba-3dab-43b6-8657-5cb44a953d8c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Neural Network Model 🏋️‍♀️🏋️‍♂️\n",
        "\n",
        "With our neural network model defined, the next exciting step is to train it. This process involves feeding the training data to the model and adjusting the model parameters (weights and biases) based on the computed loss and the chosen optimization algorithm.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Loss Function**: Measures how well the model performs. A common choice for classification tasks is Cross-Entropy Loss.\n",
        "- **Optimizer**: Helps in updating the model parameters based on the computed gradients. We'll be using Stochastic Gradient Descent (SGD) in this example.\n",
        "- **Epochs**: One epoch means the model has seen the entire dataset once. Training for multiple epochs means going through the dataset multiple times.\n",
        "\n"
      ],
      "metadata": {
        "id": "J9VnJDBDz2QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "# Import optimizer\n",
        "from torch.optim import SGD\n",
        "\n",
        "\n",
        "# TODO: Define the loss function and optimizer\n",
        "criterion =nn.CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# TODO: Set the number of epochs\n",
        "num_epochs =10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # TODO: evaluate on the test_loader\n",
        "    test_loss = 0.0\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Complete evaluation pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "    else:\n",
        "        print(f\" Loss: {test_loss/len(test_loader)}\")\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "id": "NqXh40htz2B6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e090084a-bc20-422d-a474-8b99478bb1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.317570639492225\n",
            " Loss: 0.40004401257748057\n",
            "Epoch 2, Loss: 0.3118408035669627\n",
            " Loss: 0.32226775886763814\n",
            "Epoch 3, Loss: 0.3083542331949925\n",
            " Loss: 0.36989566122602885\n",
            "Epoch 4, Loss: 0.3056124110203753\n",
            " Loss: 0.2857994048731627\n",
            "Epoch 5, Loss: 0.30441292845554696\n",
            " Loss: 0.3061939826365679\n",
            "Epoch 6, Loss: 0.3034819480913407\n",
            " Loss: 0.32723684109462675\n",
            "Epoch 7, Loss: 0.3045578286973144\n",
            " Loss: 0.3753653633271813\n",
            "Epoch 8, Loss: 0.30522877856421826\n",
            " Loss: 0.37915449941851154\n",
            "Epoch 9, Loss: 0.2999077463216746\n",
            " Loss: 0.2937800096522922\n",
            "Epoch 10, Loss: 0.2976090125580713\n",
            " Loss: 0.29498700181520576\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot the model complexity graph"
      ],
      "metadata": {
        "id": "ckEA5Ft_0gn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO1: Comment the model complexity graph\n",
        "### TODO2: Change the model and add more layer (use a complex model)"
      ],
      "metadata": {
        "id": "Rh0gKqC50ST3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PyTorch libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # TODO: Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(784,256 ) # First fully connected layer\n",
        "        self.fc2 = nn.Linear(256,128 )\n",
        "        self.fc3 = nn.Linear(128,10 )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)  # TODO: add an activation function\n",
        "        x = F.relu(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x =F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "cAstg0q6PuHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6df00ae-6cee-4363-e69a-03676f05bc36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "# Import optimizer\n",
        "from torch.optim import SGD\n",
        "\n",
        "\n",
        "# TODO: Define the loss function and optimizer\n",
        "criterion =nn.CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# TODO: Set the number of epochs\n",
        "num_epochs =20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # TODO: evaluate on the test_loader\n",
        "    test_loss = 0.0\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Complete evaluation pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "    else:\n",
        "        print(f\" Loss: {test_loss/len(test_loader)}\")\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "id": "SpA9op4rPxyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a907159a-9740-48b8-f5ab-c25b0bc4e864"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.005752749406507483\n",
            " Loss: 0.07250975104170286\n",
            "Epoch 2, Loss: 0.006449911077194703\n",
            " Loss: 0.07291533469129242\n",
            "Epoch 3, Loss: 0.0045316212949079015\n",
            " Loss: 0.07112634171798232\n",
            "Epoch 4, Loss: 0.0032975018814122565\n",
            " Loss: 0.06735397260079\n",
            "Epoch 5, Loss: 0.0019133148490276157\n",
            " Loss: 0.06662699377769721\n",
            "Epoch 6, Loss: 0.00213255827696197\n",
            " Loss: 0.06725587778842027\n",
            "Epoch 7, Loss: 0.0013493871558068683\n",
            " Loss: 0.07197256504951413\n",
            "Epoch 8, Loss: 0.0017550431907102102\n",
            " Loss: 0.07160960813009958\n",
            "Epoch 9, Loss: 0.0008700578639874138\n",
            " Loss: 0.07224944116239586\n",
            "Epoch 10, Loss: 0.0010081380153980314\n",
            " Loss: 0.07277647795535958\n",
            "Epoch 11, Loss: 0.0006327422604649878\n",
            " Loss: 0.07093196414770131\n",
            "Epoch 12, Loss: 0.0007780643419330526\n",
            " Loss: 0.07228402411775896\n",
            "Epoch 13, Loss: 0.0006750607391503335\n",
            " Loss: 0.0740413475639307\n",
            "Epoch 14, Loss: 0.0004586162098744324\n",
            " Loss: 0.07317676467861142\n",
            "Epoch 15, Loss: 0.0003894092628463292\n",
            " Loss: 0.0743005623933007\n",
            "Epoch 16, Loss: 0.0004550565212892481\n",
            " Loss: 0.07304628175999238\n",
            "Epoch 17, Loss: 0.000372821458242144\n",
            " Loss: 0.07395036378342042\n",
            "Epoch 18, Loss: 0.0003223601762515219\n",
            " Loss: 0.07526025113081088\n",
            "Epoch 19, Loss: 0.0003146713130975558\n",
            " Loss: 0.0752674829654866\n",
            "Epoch 20, Loss: 0.0002871651881464352\n",
            " Loss: 0.07531366733739812\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Early Stopping 🛑\n",
        "\n",
        "One of the key techniques in training neural networks effectively is 'Early Stopping'. This technique halts the training process if the model performance stops improving on a held-out validation set. Early stopping is a form of regularization used to avoid overfitting.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Validation Loss**: Monitor the loss on a validation set to detect when it begins to increase, indicating overfitting."
      ],
      "metadata": {
        "id": "AzCT0HIA0nQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code to implement Early stopping\n",
        "patience = 3\n",
        "min_delta = 0.01\n",
        "best_loss = None\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Training pass\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # evaluation phase\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            output = model(images)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    training_loss = running_loss / len(train_loader)\n",
        "    validation_loss /= len(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if best_loss is None or validation_loss < best_loss - min_delta:\n",
        "        best_loss = validation_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "id": "gDm6xmNB0mio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493e914c-f9ef-42b1-bef6-126134f35780"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.0001847588073771257, Validation Loss: 0.00047103059478104115\n",
            "Epoch 2, Training Loss: 0.0001806932147625028, Validation Loss: 0.00021229659614618868\n",
            "Epoch 3, Training Loss: 0.000175263833978103, Validation Loss: 9.418222907697782e-05\n",
            "Epoch 4, Training Loss: 0.00017001310016225815, Validation Loss: 3.23802960338071e-05\n",
            "Early stopping triggered!\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Answer this questions\n",
        "# What does min_delta and patience refer to ?\n",
        "# What is different from the first training ?"
      ],
      "metadata": {
        "id": "BW3uWpcW1RJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patience** is a paramers used to specify the number of consecutive epochs with no improvement in the validation loss before training is stopped\n",
        "\n",
        "**Min_delta** is a parameter used to define the minimum change in the validation loss"
      ],
      "metadata": {
        "id": "3dataMIlkkz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimenting with Dropout 🌧️\n",
        "\n",
        "Dropout is a regularization technique that temporarily drops units (along with their connections) from the neural network during training. This prevents units from co-adapting too much and helps the model to generalize better to unseen data.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Dropout Rate**: The probability of a neuron being dropped during training. Common rates are 0.2, 0.5, etc.\n",
        "- **Generalization**: Dropout improves the generalization of the model on the test data.\n"
      ],
      "metadata": {
        "id": "Lg47wh8z1xvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetWithDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWithDropout, self).__init__()\n",
        "        # Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.dropout1 = nn.Dropout(p=0.2)  # Dropout layer with 20% probability\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)  # Dropout layer with 50% probability\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # Forward pass with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network with dropout\n",
        "model_with_dropout = NetWithDropout()\n",
        "print(model_with_dropout)"
      ],
      "metadata": {
        "id": "DjioUiV31brx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1288d4fd-b6f2-4245-c579-3298ed73830e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetWithDropout(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (dropout1): Dropout(p=0.2, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train the dropout model\n",
        "# What do you notice ?"
      ],
      "metadata": {
        "id": "VBR_MqON15ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs =10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss =criterion(outputs,labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2F2gtQEwEkE",
        "outputId": "f0360a03-7a10-4beb-9ae3-1bdb23365e4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0001661985941034258\n",
            "Epoch 2, Loss: 0.00016045922787848572\n",
            "Epoch 3, Loss: 0.00015796959084335332\n",
            "Epoch 4, Loss: 0.00015358168136264352\n",
            "Epoch 5, Loss: 0.00015048369456637184\n",
            "Epoch 6, Loss: 0.00014528210868408693\n",
            "Epoch 7, Loss: 0.00014349422547327095\n",
            "Epoch 8, Loss: 0.00013974140595420206\n",
            "Epoch 9, Loss: 0.00013667499299716457\n",
            "Epoch 10, Loss: 0.00013404696047111743\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submitting Your Project on GitHub 🚀\n",
        "\n",
        "Submitting your project on GitHub not only allows you to showcase your work but also helps in version control and collaboration. Here's how you can do it:\n",
        "\n",
        "### Step 1: Create a New Repository on GitHub\n",
        "1. **Sign in to GitHub**: Go to [GitHub](https://github.com) and sign in with your account.\n",
        "2. **Create a New Repository**: Click on the '+' icon in the top right corner and select 'New repository'.\n",
        "3. **Name Your Repository**: Give your repository a meaningful name, like 'handwritten-digit-recognition'.\n",
        "4. **Initialize with a README**: Check the box 'Initialize this repository with a README'.\n",
        "5. **Create Repository**: Click the 'Create repository' button.\n",
        "\n",
        "### Step 2: Clone the Repository to Your Local Machine\n",
        "1. **Copy the Repository URL**: On your repository page on GitHub, click the 'Code' button and copy the URL.\n",
        "2. **Clone in Terminal**: Open your terminal, navigate to where you want the repository, and run `git clone [URL]`, replacing `[URL]` with the URL you copied.\n",
        "\n",
        "### Step 3: Add Your Project to the Repository\n",
        "1. **Copy Your Notebook**: Place your Jupyter notebook file into the cloned repository folder on your local machine.\n",
        "2. **Add the File**: Run `git add [filename]` in your terminal, replacing `[filename]` with the name of your notebook file.\n",
        "\n",
        "### Step 4: Commit and Push Your Changes\n",
        "1. **Commit Your Changes**: Run `git commit -m \"Add project notebook\"`.\n",
        "2. **Push to GitHub**: Run `git push` to push your changes to the GitHub repository.\n",
        "\n",
        "### Step 5: Create and Edit the README File\n",
        "1. **Edit README.md**: On GitHub, open the README.md file and click the pencil icon to edit.\n",
        "2. **Write Your README**: Include a project title, a brief description, installation instructions, and usage instructions. Optionally, add screenshots or additional sections as needed.\n",
        "3. **Save Changes**: After editing, commit your changes by clicking 'Commit changes' at the bottom.\n",
        "\n",
        "### 📌 TODOs for Submission:\n",
        "- Ensure your Jupyter notebook is well-commented and formatted.\n",
        "- Write a clear, concise README that effectively describes your project.\n",
        "- Double-check that all files have been committed and pushed to your GitHub repository.\n",
        "\n",
        "---\n",
        "\n",
        "Remember, a well-documented GitHub repository not only reflects your technical skills but also your ability to communicate and present your work effectively. Happy coding and best of luck with your project submission! 🌟👩‍💻👨‍💻\n"
      ],
      "metadata": {
        "id": "TaKBLDTE15Iy"
      }
    }
  ]
}